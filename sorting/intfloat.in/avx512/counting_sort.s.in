// Copyright (C) 2022 Sneller, Inc.
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Affero General Public License for more details.
//
// You should have received a copy of the GNU Affero General Public License
// along with this program.  If not, see <http://www.gnu.org/licenses/>.

//+build !noasm !appengine

#include "../vm/bc_imm_amd64.h"

// Code generated by generator.go; DO NOT EDIT.

#define LoadKeysAndIndices      \
    MOVQ keys+0(FP), Keys       \
    MOVQ indices+8(FP), Indices

{{ range . }}
// func countingSort{{.Suffix}}(keys *{{.KeyType}}, indices *{{.IndexType}}, size int) (sorted bool)
TEXT Â·countingSort{{.Suffix}}(SB), 7, $320-25

#define Keys            SI
#define KeysEnd         R8
#define Indices         DI
#define Count           CX
#define IndicesCopy     BX

    MOVQ size+16(FP), Count

    MOVB $1, sorted+24(FP)

    CMPQ Count, $(4*{{.KeyElements}})
    JNBE not_supported

    CMPQ Count, $(3*{{.KeyElements}})
    JA sort4registers

    CMPQ Count, $(2*{{.KeyElements}})
    JA sort3registers

    CMPQ Count, ${{.KeyElements}}
    JA sort2registers

    // Count = 0..8
sort1register:
    TESTQ Count, Count
    JZ    noelements

    // calculate active mask: K1 := (1 << size) - 1
    XORQ DX,    DX
    BTSQ Count, DX
    SUBQ $1,    DX
    KMOVQ DX,   K1

    LoadKeysAndIndices

    // copy indices to the stack ([8]uint64)
    LEAQ 0(SP), IndicesCopy
    {{.VecLoadIdx}} (Indices), Z0
    {{.VecStoreIdx}} Z0, (IndicesCopy)

    // zero offsets table (offsets type is byte[8])
    VPXORD Z0, Z0, Z0
    VMOVDQU32 X0, 128(SP)

    // load keys into zmm0
    {{.VecLoadKey}}.Z (Keys), K1, Z0

    // the address of past-last item in the keys
    LEAQ (Keys)(Count*{{.KeySize}}), KeysEnd
sort1:
    // count how many keys are less than this one
    {{.VbroadcastMem}} (Keys), Z2
    ADDQ ${{.KeySize}}, Keys

    {{.Vcmp}} ${{.CmpConstant}}, Z2, Z0, K1, K2
    KMOVW K2, R12
    POPCNTQ R12, AX

    // load the corresponding index
    {{.LoadIdx}} (IndicesCopy), R15
    ADDQ ${{.IndexSize}}, IndicesCopy

    // load the number of keys equal to this one that have been already stored
    LEAQ    128(SP)(AX*1), DX
    MOVBQZX (DX), CX
    INCB    (DX)  // increment the counter to properly handle next keys equal to this one

    // adjust the offset
    ADDQ    CX, AX

    // store index at the final destination
    {{.StoreIdx}} R15, (Indices)(AX*{{.IndexSize}})

    CMPQ Keys, KeysEnd
    JNE sort1
noelements:
    VZEROUPPER
    RET

    // Count = 9..16
sort2registers:

    // calculate active mask: K1 := (1 << (size - 8)) - 1
    LEAQ -{{.KeyElements}}(Count), AX
    XORQ DX,  DX
    BTSQ AX,  DX
    SUBQ $1,  DX
    KMOVQ DX, K1

    LoadKeysAndIndices

    // copy indices to the stack ([16]uint64)
    LEAQ 0(SP), IndicesCopy
    {{.VecLoadIdx}}   (Indices), Z0
    {{.VecLoadIdx}} 64(Indices), K1, Z1
    {{.VecStoreIdx}} Z0,   (IndicesCopy)
    {{.VecStoreIdx}} Z1, 64(IndicesCopy)

    // zero offsets table (offsets type is byte[16])
    VPXORD Z0, Z0, Z0
    VMOVDQU32 Y0, 128(SP)

    // load keys into zmm0 and zmm1
    {{.VecLoadKey}}     (Keys), Z0
    {{.VecLoadKey}}.Z 64(Keys), K1, Z1

    // the address of past-last item in the keys
    LEAQ (Keys)(Count*{{.KeySize}}), KeysEnd
sort2:
    // count how many keys are less than this one
    {{.VbroadcastMem}} (Keys), Z2
    ADDQ ${{.KeySize}}, Keys
    {{.Vcmp}} ${{.CmpConstant}}, Z2, Z0, K2
    {{.Vcmp}} ${{.CmpConstant}}, Z2, Z1, K1, K3
    KMOVW K2, R12
    KMOVW K3, R13
    POPCNTQ R12, AX
    POPCNTQ R13, CX
    ADDQ    CX, AX

    // load the corresponding index
    {{.LoadIdx}} (IndicesCopy), R15
    ADDQ ${{.IndexSize}}, IndicesCopy

    // load the number of keys equal to this one that have been already stored
    LEAQ    128(SP)(AX*1), DX
    MOVBQZX (DX), CX
    INCB    (DX)  // increment the counter to properly handle next keys equal to this one

    // adjust the offset
    ADDQ    CX, AX

    // store index at the final destination
    {{.StoreIdx}} R15, (Indices)(AX*{{.IndexSize}})

    CMPQ Keys, KeysEnd
    JNE sort2
    VZEROUPPER
    RET

    // Count = 17..24
sort3registers:

    // calculate active mask: K1 := (1 << (size - 16)) - 1
    LEAQ -{{mul 2 .KeyElements}}(Count), AX
    XORQ DX,  DX
    BTSQ AX,  DX
    SUBQ $1,  DX
    KMOVQ DX, K1

    LoadKeysAndIndices

    // copy indices to the stack ([24]uint32)
    LEAQ 0(SP), IndicesCopy
    {{.VecLoadIdx}}    (Indices), Z0
    {{.VecLoadIdx}}  64(Indices), Z1
    {{.VecLoadIdx}} 128(Indices), K1, Z2
    {{.VecStoreIdx}} Z0,    (IndicesCopy)
    {{.VecStoreIdx}} Z1,  64(IndicesCopy)
    {{.VecStoreIdx}} Z2, 128(IndicesCopy)

    // zero offsets table (offsets type is [24]byte)
    VPXORD Z0, Z0, Z0
    VMOVDQU32 Z0, 192(SP)

    // load keys
    {{.VecLoadKey}}      (Keys), Z0
    {{.VecLoadKey}}    64(Keys), Z1
    {{.VecLoadKey}}.Z 128(Keys), K1, Z2

    // the address of past-last item in the keys
    LEAQ (Keys)(Count*{{.KeySize}}), KeysEnd
sort3:
    // count how many keys are less than this one
    {{.VbroadcastMem}} (Keys), Z3
    ADDQ ${{.KeySize}}, Keys
    {{.Vcmp}} ${{.CmpConstant}}, Z3, Z0, K2
    {{.Vcmp}} ${{.CmpConstant}}, Z3, Z1, K3
    {{.Vcmp}} ${{.CmpConstant}}, Z3, Z2, K1, K4
    KMOVW K2, AX
    KMOVW K3, CX
    KMOVW K4, DX
    POPCNTQ AX, AX
    POPCNTQ CX, CX
    POPCNTQ DX, DX
    ADDQ    CX, AX
    ADDQ    DX, AX

    // load the corresponding index
    {{.LoadIdx}} (IndicesCopy), R15
    ADDQ ${{.IndexSize}}, IndicesCopy

    // load the number of keys equal to this one that have been already stored
    LEAQ    192(SP)(AX*1), DX
    MOVBQZX (DX), CX
    INCB    (DX)  // increment the counter to properly handle next keys equal to this one

    // adjust the offset
    ADDQ    CX, AX

    // store index at the final destination
    {{.StoreIdx}}  R15, (Indices)(AX*{{.IndexSize}})

    CMPQ Keys, KeysEnd
    JNE sort3
    VZEROUPPER
    RET

    // Count = 25..32
sort4registers:

    // calculate active mask: K1 := (1 << (size - 24)) - 1
    LEAQ -{{mul 3 .KeyElements}}(Count), AX
    XORQ DX,  DX
    BTSQ AX,  DX
    SUBQ $1,  DX
    KMOVQ DX, K1

    LoadKeysAndIndices

    // copy indices to the stack ([32]uint64)
    LEAQ 0(SP), IndicesCopy
    {{.VecLoadIdx}}    (Indices), Z0
    {{.VecLoadIdx}}  64(Indices), Z1
    {{.VecLoadIdx}} 128(Indices), Z2
    {{.VecLoadIdx}} 192(Indices), K1, Z3
    {{.VecStoreIdx}} Z0,    (IndicesCopy)
    {{.VecStoreIdx}} Z1,  64(IndicesCopy)
    {{.VecStoreIdx}} Z2, 128(IndicesCopy)
    {{.VecStoreIdx}} Z3, 192(IndicesCopy)

    // zero offsets table (offsets type is [32]byte)
    VPXORD Z0, Z0, Z0
    VMOVDQU32 Z0, 256(SP)

    // load keys
    {{.VecLoadKey}}      (Keys), Z0
    {{.VecLoadKey}}    64(Keys), Z1
    {{.VecLoadKey}}   128(Keys), Z2
    {{.VecLoadKey}}.Z 192(Keys), K1, Z3

    // the address of past-last item in the keys
    LEAQ (Keys)(Count*{{.KeySize}}), KeysEnd
sort4:
    // count how many keys are less than this one
    {{.VbroadcastMem}} (Keys), Z4
    ADDQ ${{.KeySize}}, Keys
    {{.Vcmp}} ${{.CmpConstant}}, Z4, Z0, K2
    {{.Vcmp}} ${{.CmpConstant}}, Z4, Z1, K3
    {{.Vcmp}} ${{.CmpConstant}}, Z4, Z2, K4
    {{.Vcmp}} ${{.CmpConstant}}, Z4, Z3, K1, K5
    KMOVW K2, AX
    KMOVW K3, CX
    KMOVW K4, DX
    KMOVW K5, R9
    POPCNTQ AX, AX
    POPCNTQ CX, CX
    POPCNTQ DX, DX
    POPCNTQ R9, R9
    ADDQ    CX, AX
    ADDQ    R9, DX
    ADDQ    DX, AX

    // load the corresponding index
    {{.LoadIdx}} (IndicesCopy), R15
    ADDQ ${{.IndexSize}}, IndicesCopy

    // load the number of keys equal to this one that have been already stored
    LEAQ    256(SP)(AX*1), DX
    MOVBQZX (DX), CX
    INCB    (DX)  // increment the counter to properly handle next keys equal to this one

    // adjust the offset
    ADDQ    CX, AX

    // store index at the final destination
    {{.StoreIdx}}  R15, (Indices)(AX*{{.IndexSize}})

    CMPQ Keys, KeysEnd
    JNE sort4
    VZEROUPPER
    RET

not_supported:
    MOVB $0, sorted+24(FP)
    RET
#undef Keys
#undef KeysEnd
#undef Indices
#undef Count
#undef IndicesCopy
{{ end }}
