// Copyright 2023 Sneller, Inc.
//
//  Licensed under the Apache License, Version 2.0 (the "License");
//  you may not use this file except in compliance with the License.
//  You may obtain a copy of the License at
//
//    http://www.apache.org/licenses/LICENSE-2.0
//
//  Unless required by applicable law or agreed to in writing, software
//  distributed under the License is distributed on an "AS IS" BASIS,
//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//  See the License for the specific language governing permissions and
//  limitations under the License.

TEXT sinf32(SB), NOSPLIT|NOFRAME, $0
  //IN Z2 = 16 float32
  //IN K1 = active mask
  VEXTRACTF32X8  $1, Z0, Y3
  VCVTPS2PD Y0, Z2
  VCVTPS2PD Y3, Z3
  KSHIFTRW $8, K1, K2

  KTESTW K1, K1
  JZ skip

  // case 'a': this case is implicit and is always executed
  VBROADCASTSD CONSTF64_ABS_BITS(), Z7
  VBROADCASTSD CONST_GET_PTR(const_sin, 0), Z4
  VBROADCASTSD CONST_GET_PTR(const_sin, 8), Z5

  VANDPD Z7, Z2, Z6
  VANDPD Z7, Z3, Z7

  // K3/K4 contain lanes where x >= 15, which require more work
  VCMPPD $VCMP_IMM_GE_OQ, Z4, Z6, K1, K3
  VCMPPD $VCMP_IMM_GE_OQ, Z4, Z7, K2, K4
  KUNPCKBW K3, K4, K3

  VMULPD Z5, Z2, Z4
  VMULPD Z5, Z3, Z5

  VRNDSCALEPD $8, Z4, Z8
  VRNDSCALEPD $8, Z5, Z9
  VCVTPD2DQ.RN_SAE Z8, Y4
  VCVTPD2DQ.RN_SAE Z9, Y5
  VINSERTI32X8 $1, Y5, Z4, Z4

  VBROADCASTSD CONST_GET_PTR(const_sin, 16), Z10
  VBROADCASTSD CONST_GET_PTR(const_sin, 24), Z12
  VMOVAPD Z10, Z11
  VFMADD213PD Z2, Z8, Z10 // Z10 = (Z8 * Z10) + Z2
  VFMADD213PD Z3, Z9, Z11 // Z11 = (Z9 * Z11) + Z3
  VMULPD Z12, Z8, Z8
  VMULPD Z12, Z9, Z9
  VADDPD Z8, Z10, Z14
  VADDPD Z9, Z11, Z15
  VSUBPD Z14, Z10, Z10
  VSUBPD Z15, Z11, Z11
  VADDPD Z10, Z8, Z16
  VADDPD Z11, Z9, Z17

  // Jump to 'sin_case_b' if one or more lane has x >= 15
  KTESTW K3, K3
  JNE sin_case_b

sin_eval_poly:
  // Polynomial evaluation; code shared by all cases
  VMULPD Z14, Z14, Z6
  VMULPD Z15, Z15, Z7
  VADDPD Z14, Z14, Z8
  VADDPD Z15, Z15, Z9

  VMOVAPD Z14, Z10
  VMOVAPD Z15, Z11
  VFMSUB213PD Z6, Z14, Z10 // Z10 = (Z14 * Z10) - Z6
  VFMSUB213PD Z7, Z15, Z11 // Z11 = (Z15 * Z11) - Z7
  VFMADD231PD Z8, Z16, Z10 // Z10 = (Z16 * Z8) + Z10
  VFMADD231PD Z9, Z17, Z11 // Z11 = (Z17 * Z9) + Z11

  VMULPD Z6, Z6, Z8
  VMULPD Z7, Z7, Z9
  VMULPD Z8, Z8, Z12
  VMULPD Z9, Z9, Z13

  VBROADCASTSD CONST_GET_PTR(const_sin, 152), Z18
  VBROADCASTSD CONST_GET_PTR(const_sin, 168), Z20
  VBROADCASTSD CONST_GET_PTR(const_sin, 184), Z22
  VMOVAPD Z18, Z19
  VMOVAPD Z20, Z21
  VMOVAPD Z22, Z23

  VBROADCASTSD CONST_GET_PTR(const_sin, 160), Z24
  VBROADCASTSD CONST_GET_PTR(const_sin, 176), Z25
  VFMADD213PD Z24, Z6, Z18 // Z18 = (Z6 * Z18) + Z24
  VFMADD213PD Z24, Z7, Z19 // Z19 = (Z7 * Z19) + Z24
  VFMADD213PD Z25, Z6, Z20 // Z20 = (Z6 * Z20) + Z25
  VFMADD213PD Z25, Z7, Z21 // Z21 = (Z7 * Z21) + Z25

  VBROADCASTSD CONST_GET_PTR(const_sin, 192), Z24
  VBROADCASTSD CONST_GET_PTR(const_sin, 200), Z25
  VFMADD213PD Z24, Z6, Z22  // Z22 = (Z6 * Z22) + Z24
  VFMADD213PD Z24, Z7, Z23  // Z23 = (Z7 * Z23) + Z24
  VFMADD231PD Z20, Z8, Z22  // Z22 = (Z8 * Z20) + Z22
  VFMADD231PD Z21, Z9, Z23  // Z23 = (Z9 * Z21) + Z23
  VFMADD231PD Z18, Z12, Z22 // Z22 = (Z12 * Z18) + Z22
  VFMADD231PD Z19, Z13, Z23 // Z23 = (Z13 * Z19) + Z23
  VFMADD213PD Z25, Z6, Z22  // Z22 = (Z6 * Z22) + Z25
  VFMADD213PD Z25, Z7, Z23  // Z23 = (Z7 * Z23) + Z25

  VBROADCASTSD CONST_GET_PTR(const_sin, 208), Z13
  VMULPD Z22, Z6, Z8
  VMULPD Z23, Z7, Z9
  VADDPD Z13, Z8, Z18
  VADDPD Z13, Z9, Z19
  VSUBPD Z18, Z13, Z12
  VSUBPD Z19, Z13, Z13
  VADDPD Z12, Z8, Z8
  VADDPD Z13, Z9, Z9
  VMULPD Z18, Z6, Z12
  VMULPD Z19, Z7, Z13
  VMOVAPD Z6, Z20
  VMOVAPD Z7, Z21
  VFMSUB213PD Z12, Z18, Z20 // Z20 = (Z18 * Z20) - Z12
  VFMSUB213PD Z13, Z19, Z21 // Z21 = (Z19 * Z21) - Z13
  VFMADD231PD Z8, Z6, Z20   // Z20 = (Z6 * Z8) + Z20
  VFMADD231PD Z9, Z7, Z21   // Z21 = (Z7 * Z9) + Z21
  VFMADD231PD Z10, Z18, Z20 // Z20 = (Z18 * Z10) + Z20
  VFMADD231PD Z11, Z19, Z21 // Z21 = (Z19 * Z11) + Z21

  VBROADCASTSD CONSTF64_1(), Z7
  VADDPD Z7, Z12, Z8
  VADDPD Z7, Z13, Z9
  VSUBPD Z8, Z7, Z6
  VSUBPD Z9, Z7, Z7
  VADDPD Z6, Z12, Z6
  VADDPD Z7, Z13, Z7
  VADDPD Z20, Z6, Z6
  VADDPD Z21, Z7, Z7
  VMULPD Z6, Z14, Z6
  VMULPD Z7, Z15, Z7
  VFMADD231PD Z16, Z8, Z6 // Z6 = (Z8 * Z16) + Z6
  VFMADD231PD Z17, Z9, Z7 // Z7 = (Z9 * Z17) + Z7
  VFMADD231PD Z8, Z14, Z6 // Z6 = (Z14 * Z8) + Z6
  VFMADD231PD Z9, Z15, Z7 // Z7 = (Z15 * Z9) + Z7

  VPANDD.BCST CONST_GET_PTR(const_sin, 128), Z4, Z4
  VPCMPEQD.BCST CONST_GET_PTR(const_sin, 240), Z4, K3
  KSHIFTRW $8, K3, K4
  VPBROADCASTQ.Z CONSTF64_SIGN_BIT(), K3, Z4
  VPBROADCASTQ.Z CONSTF64_SIGN_BIT(), K4, Z5
  VXORPD Z6, Z4, Z4
  VXORPD Z7, Z5, Z5
  VXORPD X6, X6, X6
  VCMPPD $VCMP_IMM_EQ_OQ, Z6, Z2, K3
  VCMPPD $VCMP_IMM_EQ_OQ, Z6, Z3, K4
  VMOVAPD Z2, K3, Z4
  VMOVAPD Z3, K4, Z5

	//OUT Z2 = 16 float32
	//OUT K1 = active lanes
	VCVTPD2PS Z4, Y0
	VCVTPD2PS Z5, Y5
	VINSERTF32X8 $1, Y5, Z0, Z0
	RET

	//BC_UNPACK_2xSLOT(0, OUT(DX), OUT(R8))
	//BC_STORE_F64_TO_SLOT(IN(Z4), IN(Z5), IN(DX))
	//BC_STORE_K_TO_SLOT(IN(K1), IN(R8))
	//NEXT_ADVANCE(BC_SLOT_SIZE*4)

sin_case_b:
  // case 'b': one or more lane has x >= 1e14
  VBROADCASTSD CONST_GET_PTR(const_sin, 32), Z9
  VMULPD Z9, Z2, Z8
  VMULPD Z9, Z3, Z9
  VRNDSCALEPD $11, Z8, Z8
  VRNDSCALEPD $11, Z9, Z9
  VBROADCASTSD CONST_GET_PTR(const_sin, 8), Z10
  VBROADCASTSD CONST_GET_PTR(const_sin, 40), Z11
  VMULPD Z11, Z8, Z8
  VMULPD Z11, Z9, Z9
  VMOVAPD Z10, Z11
  VFMSUB213PD Z8, Z2, Z10 // Z10 = (Z2 * Z10) - Z8
  VFMSUB213PD Z9, Z3, Z11 // Z11 = (Z3 * Z11) - Z9
  VRNDSCALEPD $8, Z10, Z12
  VRNDSCALEPD $8, Z11, Z13
  VBROADCASTSD CONST_GET_PTR(const_sin, 48), Z10
  VBROADCASTSD CONST_GET_PTR(const_sin, 48), Z11
  VMULPD Z10, Z12, Z18
  VMULPD Z11, Z13, Z19
  VFMADD213PD Z2, Z8, Z10 // Z10 = (Z8 * Z10) + Z2
  VFMADD213PD Z3, Z9, Z11 // Z11 = (Z9 * Z11) + Z3
  VADDPD Z18, Z10, Z20
  VADDPD Z19, Z11, Z21
  VSUBPD Z20, Z10, Z10
  VSUBPD Z21, Z11, Z11
  VADDPD Z10, Z18, Z10
  VADDPD Z11, Z19, Z11

  VBROADCASTSD CONST_GET_PTR(const_sin, 56), Z19
  VMULPD Z19, Z8, Z22
  VMULPD Z19, Z9, Z23
  VADDPD Z20, Z22, Z24
  VADDPD Z21, Z23, Z25
  VSUBPD Z20, Z24, Z26
  VSUBPD Z21, Z25, Z27
  VSUBPD Z26, Z24, Z5
  VSUBPD Z27, Z25, Z18
  VSUBPD Z5, Z20, Z20
  VSUBPD Z18, Z21, Z21
  VSUBPD Z26, Z22, Z22
  VSUBPD Z27, Z23, Z23
  VADDPD Z20, Z22, Z20
  VADDPD Z21, Z23, Z21
  VADDPD Z20, Z10, Z10
  VADDPD Z21, Z11, Z11
  VMULPD Z19, Z12, Z18
  VMULPD Z19, Z13, Z19
  VADDPD Z24, Z18, Z20
  VADDPD Z25, Z19, Z21
  VSUBPD Z24, Z20, Z22
  VSUBPD Z25, Z21, Z23
  VSUBPD Z22, Z20, Z26
  VSUBPD Z23, Z21, Z27
  VSUBPD Z26, Z24, Z24
  VSUBPD Z27, Z25, Z25
  VSUBPD Z22, Z18, Z18
  VSUBPD Z23, Z19, Z19
  VADDPD Z24, Z18, Z18
  VADDPD Z25, Z19, Z19

  VBROADCASTSD CONST_GET_PTR(const_sin, 64), Z23
  VADDPD Z10, Z18, Z10
  VADDPD Z11, Z19, Z11
  VMULPD Z23, Z8, Z18
  VMULPD Z23, Z9, Z19
  VADDPD Z20, Z18, Z24
  VADDPD Z21, Z19, Z25
  VSUBPD Z20, Z24, Z26
  VSUBPD Z21, Z25, Z27
  VSUBPD Z26, Z24, Z5
  VSUBPD Z27, Z25, Z22
  VSUBPD Z5, Z20, Z20
  VSUBPD Z22, Z21, Z21
  VSUBPD Z26, Z18, Z18
  VSUBPD Z27, Z19, Z19
  VADDPD Z20, Z18, Z18
  VADDPD Z21, Z19, Z19
  VADDPD Z10, Z18, Z10
  VADDPD Z11, Z19, Z11
  VMULPD Z23, Z12, Z18
  VMULPD Z23, Z13, Z19
  VADDPD Z24, Z18, Z20
  VADDPD Z25, Z19, Z21
  VSUBPD Z24, Z20, Z22
  VSUBPD Z25, Z21, Z23
  VSUBPD Z22, Z20, Z26
  VSUBPD Z23, Z21, Z27
  VSUBPD Z26, Z24, Z24
  VSUBPD Z27, Z25, Z25
  VSUBPD Z22, Z18, Z18
  VSUBPD Z23, Z19, Z19
  VADDPD Z24, Z18, Z18
  VADDPD Z25, Z19, Z19
  VADDPD Z10, Z18, Z10
  VADDPD Z11, Z19, Z11
  VBROADCASTSD CONST_GET_PTR(const_sin, 72), Z19
  VADDPD Z12, Z8, Z8
  VADDPD Z13, Z9, Z9
  VMULPD Z19, Z8, Z18
  VMULPD Z19, Z9, Z19
  VADDPD Z20, Z18, Z8
  VADDPD Z21, Z19, Z9
  VSUBPD Z8, Z20, Z20
  VSUBPD Z9, Z21, Z21
  VADDPD Z20, Z18, Z18
  VADDPD Z21, Z19, Z19
  VADDPD Z10, Z18, Z10
  VADDPD Z11, Z19, Z11

  VCVTPD2DQ.RN_SAE Z12, Y18
  VCVTPD2DQ.RN_SAE Z13, Y19
  VINSERTI32X8 $1, Y19, Z18, K3, Z4

  VBROADCASTSD CONST_GET_PTR(const_sin, 80), Z20
  VMOVAPD Z8, K3, Z14
  VMOVAPD Z9, K4, Z15
  VMOVAPD Z10, K3, Z16
  VMOVAPD Z11, K4, Z17

  VCMPPD $VCMP_IMM_GE_OS, Z20, Z6, K3, K3
  VCMPPD $VCMP_IMM_GE_OS, Z20, Z7, K4, K4
  KUNPCKBW K3, K4, K3
  KTESTW K3, K3
  JZ sin_eval_poly

  // case 'c': one or more lane has x >= 1e14
  VBROADCASTSD CONSTF64_POSITIVE_INF(), Z12
  MOVL $0xAAAA, R8
  LEAQ CONST_GET_PTR(const_rempi, 0), R15

  // K0 contains mask of all inputs that are either +INF or -INF
  VCMPPD $VCMP_IMM_EQ_OQ, Z12, Z6, K3, K0
  VCMPPD $VCMP_IMM_EQ_OQ, Z12, Z7, K4, K5
  KUNPCKBW K0, K5, K0

  VGETEXPPD Z2, Z12
  VGETEXPPD Z3, Z13
  VCVTPD2DQ.RN_SAE Z12, Y8
  VCVTPD2DQ.RN_SAE Z13, Y9
  VINSERTI32X8 $1, Y9, Z8, Z8

  VPCMPGTD.BCST CONSTD_NEG_1(), Z8, K5
  VPANDD.BCST CONST_GET_PTR(const_sin, 216), Z8, Z8
  VMOVDQA32.Z Z8, K5, Z8
  VPADDD.BCST CONST_GET_PTR(const_sin, 220), Z8, Z10
  VPCMPGTD.BCST CONST_GET_PTR(const_sin, 224), Z10, K5
  VPBROADCASTD.Z CONST_GET_PTR(const_sin, 228), K5, Z8
  VPSLLD $20, Z8, Z8
  VEXTRACTI32X8 $1, Z8, Y9
  KMOVW R8, K5
  VPEXPANDD.Z Z8, K5, Z8
  VPEXPANDD.Z Z9, K5, Z9
  VPADDQ Z2, Z8, Z8
  VPADDQ Z3, Z9, Z9
  VPSRAD $31, Z10, Z18
  VPANDND Z10, Z18, Z10
  VPSLLD $2, Z10, Z10
  VEXTRACTI32X8 $1, Z10, Y11

  KMOVB K3, K5
  KMOVB K4, K6
  VXORPD X18, X18, X18
  VXORPD X19, X19, X19
  VGATHERDPD 0(R15)(Y10*8), K5, Z18
  VGATHERDPD 0(R15)(Y11*8), K6, Z19
  ADDQ $8, R15

  VMULPD Z8, Z18, Z20
  VMULPD Z9, Z19, Z21
  VFMSUB213PD Z20, Z8, Z18 // Z18 = (Z8 * Z18) - Z20
  VFMSUB213PD Z21, Z9, Z19 // Z19 = (Z9 * Z19) - Z21
  VBROADCASTSD CONST_GET_PTR(const_sin, 88), Z23
  VMULPD Z23, Z20, Z24
  VMULPD Z23, Z21, Z25
  VRNDSCALEPD $8, Z24, Z24
  VRNDSCALEPD $8, Z25, Z25
  VRNDSCALEPD $8, Z20, Z26
  VRNDSCALEPD $8, Z21, Z27
  VMULPD Z23, Z26, Z26
  VMULPD Z23, Z27, Z27
  VSUBPD Z26, Z24, Z26
  VSUBPD Z27, Z25, Z27
  VCVTPD2DQ.RZ_SAE Z26, Y26
  VCVTPD2DQ.RZ_SAE Z27, Y27
  VINSERTI32X8 $1, Y27, Z26, Z26

  VBROADCASTSD CONST_GET_PTR(const_sin, 96), Z27
  VMULPD Z27, Z24, Z24
  VMULPD Z27, Z25, Z25
  VSUBPD Z24, Z20, Z20
  VSUBPD Z25, Z21, Z21
  VADDPD Z18, Z20, Z24
  VADDPD Z19, Z21, Z25
  VSUBPD Z24, Z20, Z20
  VSUBPD Z25, Z21, Z21
  VADDPD Z20, Z18, Z18
  VADDPD Z21, Z19, Z19

  KMOVB K3, K5
  KMOVB K4, K6
  VXORPD X20, X20, X20
  VXORPD X21, X21, X21
  VGATHERDPD 0(R15)(Y10*8), K5, Z20
  VGATHERDPD 0(R15)(Y11*8), K6, Z21
  ADDQ $8, R15

  VMULPD Z8, Z20, Z12
  VMULPD Z9, Z21, Z13
  VFMSUB213PD Z12, Z8, Z20 // Z20 = (Z8 * Z20) - Z12
  VFMSUB213PD Z13, Z9, Z21 // Z21 = (Z9 * Z21) - Z13
  VADDPD Z18, Z20, Z18
  VADDPD Z19, Z21, Z19
  VADDPD Z24, Z12, Z20
  VADDPD Z25, Z13, Z21
  VSUBPD Z24, Z20, Z6
  VSUBPD Z25, Z21, Z7
  VSUBPD Z6, Z20, Z5
  VSUBPD Z5, Z24, Z24
  VSUBPD Z7, Z21, Z5
  VSUBPD Z5, Z25, Z25
  VSUBPD Z6, Z12, Z12
  VSUBPD Z7, Z13, Z13
  VADDPD Z24, Z12, Z24
  VADDPD Z25, Z13, Z25
  VADDPD Z24, Z18, Z24
  VADDPD Z25, Z19, Z25
  VMULPD Z23, Z20, Z18
  VMULPD Z23, Z21, Z19
  VRNDSCALEPD $8, Z18, Z12
  VRNDSCALEPD $8, Z19, Z13
  VRNDSCALEPD $8, Z20, Z18
  VRNDSCALEPD $8, Z21, Z19
  VMULPD Z23, Z18, Z18
  VMULPD Z23, Z19, Z19
  VSUBPD Z18, Z12, Z18
  VSUBPD Z19, Z13, Z19
  VCVTPD2DQ.RZ_SAE Z18, Y18
  VCVTPD2DQ.RZ_SAE Z19, Y19
  VBROADCASTSD CONST_GET_PTR(const_sin, 96), Z23
  VINSERTI32X8 $1, Y19, Z18, Z18
  VPADDD Z26, Z18, Z18
  VMULPD Z23, Z12, Z22
  VMULPD Z23, Z13, Z23
  VSUBPD Z22, Z20, Z20
  VSUBPD Z23, Z21, Z21
  VADDPD Z24, Z20, Z22
  VADDPD Z25, Z21, Z23
  VSUBPD Z22, Z20, Z20
  VSUBPD Z23, Z21, Z21
  VADDPD Z20, Z24, Z20
  VADDPD Z21, Z25, Z21

  KMOVB K3, K5
  KMOVB K4, K6
  VXORPD X24, X24, X24
  VXORPD X25, X25, X25
  VGATHERDPD 0(R15)(Y10*8), K5, Z24
  VGATHERDPD 0(R15)(Y11*8), K6, Z25
  ADDQ $8, R15

  KMOVB K3, K5
  KMOVB K4, K6
  VXORPD X26, X26, X26
  VXORPD X27, X27, X27
  VGATHERDPD 0(R15)(Y10*8), K5, Z26
  VGATHERDPD 0(R15)(Y11*8), K6, Z27

  VMULPD Z8, Z24, Z10
  VMULPD Z9, Z25, Z11
  VFMSUB213PD Z10, Z8, Z24 // Z24 = (Z8 * Z24) - Z10
  VFMSUB213PD Z11, Z9, Z25 // Z25 = (Z9 * Z25) - Z11
  VFMADD231PD Z26, Z8, Z24 // Z24 = (Z8 * Z26) + Z24
  VFMADD231PD Z27, Z9, Z25 // Z25 = (Z9 * Z27) + Z25
  VADDPD Z20, Z24, Z20
  VADDPD Z21, Z25, Z21
  VADDPD Z22, Z10, Z24
  VADDPD Z23, Z11, Z25
  VSUBPD Z22, Z24, Z26
  VSUBPD Z23, Z25, Z27
  VSUBPD Z26, Z24, Z12
  VSUBPD Z27, Z25, Z13
  VSUBPD Z12, Z22, Z22
  VSUBPD Z13, Z23, Z23
  VSUBPD Z26, Z10, Z10
  VSUBPD Z27, Z11, Z11
  VADDPD Z22, Z10, Z10
  VADDPD Z23, Z11, Z11
  VADDPD Z10, Z20, Z10
  VADDPD Z11, Z21, Z11
  VADDPD Z10, Z24, Z20
  VADDPD Z11, Z25, Z21
  VSUBPD Z20, Z24, Z22
  VSUBPD Z21, Z25, Z23
  VADDPD Z22, Z10, Z10
  VADDPD Z23, Z11, Z11

  VBROADCASTSD CONST_GET_PTR(const_sin, 104), Z23
  VMULPD Z23, Z20, Z24
  VMULPD Z23, Z21, Z25
  VMOVAPD Z23, Z26
  VMOVAPD Z23, Z27
  VFMSUB213PD Z24, Z20, Z26 // Z26 = (Z20 * Z26) - Z24
  VFMSUB213PD Z25, Z21, Z27 // Z27 = (Z21 * Z27) - Z25
  VFMADD231PD Z10, Z23, Z26 // Z26 = (Z23 * Z10) + Z26
  VBROADCASTSD CONST_GET_PTR(const_sin, 112), Z10
  VFMADD231PD Z11, Z23, Z27 // Z27 = (Z23 * Z11) + Z27
  VBROADCASTSD CONSTF64_ABS_BITS(), Z11
  VFMADD231PD Z10, Z20, Z26 // Z26 = (Z20 * Z10) + Z26
  VFMADD231PD Z10, Z21, Z27 // Z27 = (Z21 * Z10) + Z27

  VBROADCASTSD CONST_GET_PTR(const_sin, 120), Z5
  VANDPD Z11, Z8, Z10
  VANDPD Z11, Z9, Z11
  VCMPPD $VCMP_IMM_LT_OS, Z5, Z10, K5
  VCMPPD $VCMP_IMM_LT_OS, Z5, Z11, K6
  VMOVAPD Z8, K5, Z24
  VMOVAPD Z9, K6, Z25
  VPADDD Z18, Z18, Z8
  VPANDD.BCST CONST_GET_PTR(const_sin, 232), Z8, Z8
  VXORPD Z26, Z26, K5, Z26
  VXORPD Z27, Z27, K6, Z27

  VXORPD X12, X12, X12
  VCMPPD $VCMP_IMM_LT_OS, Z24, Z12, K5
  VCMPPD $VCMP_IMM_LT_OS, Z25, Z12, K6
  KUNPCKBW K5, K6, K5
  VPBROADCASTD CONST_GET_PTR(const_sin, 236), Z10
  VPBROADCASTD CONST_GET_PTR(const_sin, 240), Z22
  VPBLENDMD Z10, Z22, K5, Z10
  VPADDD Z10, Z8, Z8
  VPBROADCASTQ CONST_GET_PTR(const_sin, 128), Z10
  VPSRLD $2, Z8, Z20
  VPANDD Z10, Z18, Z8
  VPCMPEQD Z22, Z8, K5
  KSHIFTRW $8, K5, K6
  VBROADCASTSD CONSTF64_SIGN_BIT(), Z9
  VBROADCASTSD CONST_GET_PTR(const_sin, 136), Z11
  VANDPD Z9, Z24, Z8
  VANDPD Z9, Z25, Z9
  VBROADCASTSD CONST_GET_PTR(const_sin, 144), Z5
  VXORPD Z11, Z8, Z10
  VXORPD Z11, Z9, Z11
  VXORPD Z5, Z8, Z8
  VXORPD Z5, Z9, Z9
  VADDPD Z10, Z24, Z18
  VADDPD Z11, Z25, Z19
  VSUBPD Z24, Z18, Z22
  VSUBPD Z25, Z19, Z23
  VSUBPD Z22, Z18, Z12
  VSUBPD Z23, Z19, Z13
  VSUBPD Z12, Z24, Z12
  VSUBPD Z13, Z25, Z13
  VSUBPD Z22, Z10, Z10
  VSUBPD Z23, Z11, Z11
  VADDPD Z12, Z10, Z10
  VADDPD Z13, Z11, Z11
  VADDPD Z8, Z26, Z8
  VADDPD Z9, Z27, Z9
  VMOVAPD Z18, K5, Z24
  VMOVAPD Z19, K6, Z25
  VADDPD Z10, Z8, K5, Z26
  VADDPD Z11, Z9, K6, Z27
  VADDPD Z26, Z24, Z8
  VADDPD Z27, Z25, Z9
  VSUBPD Z8, Z24, Z10
  VSUBPD Z9, Z25, Z11
  VADDPD Z10, Z26, Z10
  VADDPD Z11, Z27, Z11

  VMOVDQA32 Z20, K3, Z4
  VMOVAPD Z8, K3, Z14
  VMOVAPD Z9, K4, Z15
  VMOVAPD Z10, K3, Z16
  VMOVAPD Z11, K4, Z17

  VXORPD X12, X12, X12
  VCMPPD $VCMP_IMM_UNORD_Q, Z12, Z2, K3, K3
  VCMPPD $VCMP_IMM_UNORD_Q, Z12, Z3, K4, K4
  KSHIFTRW $8, K0, K5
  KORW K0, K3, K3
  KORW K5, K4, K4
  VPTERNLOGQ $0xFF, Z14, Z14, K3, Z14
  VPTERNLOGQ $0xFF, Z15, Z15, K4, Z15
  JMP sin_eval_poly

skip:
	//OUT Z2 = 16 float32
	//OUT K1 = active lanes
	VCVTPD2PS Z2, Y0
	VCVTPD2PS Z3, Y3
	VINSERTF32X8 $1, Y3, Z0, Z0
	RET
//  BC_UNPACK_2xSLOT(0, OUT(DX), OUT(R8))
//  BC_STORE_F64_TO_SLOT(IN(Z2), IN(Z3), IN(DX))
//  BC_STORE_K_TO_SLOT(IN(K1), IN(R8))
//  NEXT_ADVANCE(BC_SLOT_SIZE*4)
